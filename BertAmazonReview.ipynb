{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to load amazon review data into a format compatible with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import gzip\n",
    "# import json\n",
    "# import numpy as np\n",
    "# data_path='/home/hani/Downloads/Software_5.json.gz'\n",
    "# destination_dir=\"/home/hani/Data/amazon-review\"\n",
    "# f = gzip.open(data_path, 'rb')\n",
    "# file_content = f.readline()\n",
    "# counter=0\n",
    "# while file_content:\n",
    "#     try:\n",
    "        \n",
    "#         line = json.loads(file_content)\n",
    "#         line[\"reviewText\"]!=\"\" and line[\"overall\"]!=\"\"\n",
    "        \n",
    "#         if np.random.binomial(2, 0.15, 1)[0]==0:#train\n",
    "#             f2 = open(destination_dir+\"/train/\"+str(int(line[\"overall\"]))+\"/\"+str(counter)+\".txt\" , \"w\")\n",
    "#         else:\n",
    "#             f2 = open(destination_dir+\"/test/\"+str(int(line[\"overall\"]))+\"/\"+str(counter)+\".txt\" , \"w\")\n",
    "    \n",
    "#         f2.write(line[\"reviewText\"])\n",
    "#         f2.close()\n",
    "#         file_content = f.readline()\n",
    "#         counter+=1\n",
    "#     except:\n",
    "#         file_content = f.readline()\n",
    "#         counter+=1\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf={\n",
    "    'epochs':5,\n",
    "    'init_lr':1e-5,\n",
    "    'drop_out_rate':0.05,\n",
    "    'n_classes': 5,\n",
    "    'batch_size':32\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into keras text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hani/anaconda37/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9250 files belonging to 5 classes.\n",
      "Using 7400 files for training.\n",
      "Found 9250 files belonging to 5 classes.\n",
      "Using 1850 files for validation.\n",
      "Found 3554 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "# batch_size = 32\n",
    "seed = 42\n",
    "data_path=\"/home/hani/Data/amazon-review\"\n",
    "\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    data_path+'/train',\n",
    "    batch_size=conf['batch_size'],\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    data_path+'/train',\n",
    "    batch_size=conf['batch_size'],\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    data_path+'/test',\n",
    "    batch_size=conf['batch_size'])\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b'NOT REALLY FREE. WAS ANOYED WITH THAT.'\n",
      "Label : 0 (1)\n",
      "Review: b'Works Great'\n",
      "Label : 4 (5)\n",
      "Review: b'The basic essentials are covered in this package providing word processing, spreadsheets and presentations. Installation from the USB stick is a simple task.\\n\\nCorel Home Office is a fine product for its price point. The applications look and operate much like their Microsoft Office [2007] counterparts and provide adequately comparable functionality - there are fewer transitions for presentations, fewer charts for spreadsheets, less complex word processing features, a slightly less streamlined and feature-filled interface. Core features and tools are included; most of what\\'s lacking would be missed primarily by the power user.\\n\\nIt\\'s a fine product for small-scale business or for anyone unwilling to shell out big bucks for productivity software - not a bad choice for students on a budget.\\n\\nTransitioning to and from a larger suite is easy enough: Corel Write\\'s \"Tools\" menu encapsulates some of the tools you find on Word\\'s \"Mailings\" and \"Review\" tabs; the \"Insert\" menu contains trimmed down \"Reference\" tab features.  These dissimilarities are characteristic of the entire suite.\\n\\nIf your small business does mass mailings or if you\\'re tackling any large-scale writing projects, you might do better with a more advanced suite whose convenience features could quickly add up to countless hours of saved time. If you\\'re an individual needing a productivity suite with a small footprint at a good price, I think Corel Home Office Personal & Business should satisfy.'\n",
      "Label : 3 (4)\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(3):\n",
    "        print(f'Review: {text_batch.numpy()[i]}')\n",
    "        label = label_batch.numpy()[i]\n",
    "        print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_ds))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Pre-processing and bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tensorflow-text\n",
    "# !pip install -q tf-models-official\n",
    "# !pip install pydot\n",
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import shutil\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optmizer\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1'\n",
    "tfhub_handle_encoder    = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
    "\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_word_ids', 'input_type_ids', 'input_mask']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [ 101 2009 2003 1037 2307 4031 1012  102    0    0    0    0]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 0 0 0 0]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "text_test = ['it is a great product.']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Pooled Outputs Shape:(1, 512)\n",
      "Sequence Outputs Shape:(1, 128, 512)\n"
     ]
    }
   ],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "# print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "# print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a sentiment class using last layer of bert with dropout and one classifier softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentPredictor(tf.keras.Model):\n",
    "    def __init__(self,drop_out_rate=0.1,n_classes=5,\n",
    "                 tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "                 tfhub_handle_encoder    = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'):\n",
    "        super(SentimentPredictor, self).__init__()\n",
    "        self.n_classes=n_classes\n",
    "        self.preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "        self.encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(drop_out_rate)\n",
    "        self.out_layer = tf.keras.layers.Dense(n_classes, activation=None, name='classifier')\n",
    "\n",
    "    def call(self, inp):\n",
    "        self.encoder_inputs = self.preprocessing_layer(inp)\n",
    "        self.bert_out = self.encoder(self.encoder_inputs)['pooled_output']\n",
    "        x =self.dropout_layer(self.bert_out)\n",
    "        self.final_output = self.out_layer(x)\n",
    "        return self.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       "array([[ 1.0428904, -0.7776122, -0.9397965,  1.1107899,  0.7062758]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "classifier = SentimentPredictor(conf['drop_out_rate'],conf['n_classes'])\n",
    "classifier(tf.constant(['it is a great product.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_object    = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss_ = loss_object(real, pred)\n",
    "    return tf.reduce_sum(loss_)\n",
    "\n",
    "\n",
    "\n",
    "train_acc = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "valid_acc = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "# train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "init_lr = conf['init_lr']\n",
    "\n",
    "\n",
    "epochs=conf['epochs']\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(0.1*num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HaniSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "#       def __init__(self, warmup_steps=3, initial_lr=0.001):\n",
    "#         super(HaniSchedule, self).__init__()\n",
    "\n",
    "#         self.bvalid = np.inf\n",
    "#         self.initial_lr= initial_lr\n",
    "#         self.warmup_steps = warmup_steps\n",
    "#         self.not_imp= 0\n",
    "#         self.div=1\n",
    "#       def check(self,cval):\n",
    "#         if cval<self.bvalid:\n",
    "#           self.bvalid=cval\n",
    "#           self.not_imp= 0\n",
    "#           print(self.bvalid)\n",
    "#         else:\n",
    "#           self.not_imp+=1\n",
    "#           print(self.bvalid,\".\")\n",
    "#           if self.not_imp>self.warmup_steps:\n",
    "#             self.div+=1\n",
    "#             self.not_imp=0\n",
    "#             print(self.bvalid,\"**\")\n",
    "\n",
    "#       def __call__(self, step):\n",
    "#         return self.initial_lr /self.div\n",
    "\n",
    "# lr_schedule = HaniSchedule(warmup_steps=num_train_steps*0.1,initial_lr=conf['init_lr'])\n",
    "# optimizer=tf.keras.optimizers.RMSprop(learning_rate=lr_schedule,rho=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n",
    "# met = tf.keras.metrics.Mean('tmp')\n",
    "# print(m([1, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]))\n",
    "# print(m([0, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]))\n",
    "# print(met(m.result()))\n",
    "# print(m.result().numpy())\n",
    "# m.update_state([0, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n",
    "# print(m.result().numpy())\n",
    "\n",
    "# print(met(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(classifier=classifier,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "# # if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, ), dtype=tf.string),\n",
    "    tf.TensorSpec(shape=(None, ), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = classifier(inp)\n",
    "        loss = loss_function(tar, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, classifier.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, classifier.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_acc(tar, predictions)\n",
    "    \n",
    "    \n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def valid_step(inp, tar):\n",
    "    predictions = classifier(inp)    \n",
    "    \n",
    "    valid_loss(loss_function(tar, predictions))\n",
    "    valid_acc(tar, predictions)\n",
    "\n",
    "def reset_stats():\n",
    "    train_loss.reset_states()\n",
    "    train_acc.reset_states()\n",
    "    \n",
    "    valid_loss.reset_states()\n",
    "    valid_acc.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_val_loss=np.inf\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     start = time.time()\n",
    "\n",
    "#     reset_stats()\n",
    "\n",
    "#     for (batch, (inp, tar)) in enumerate(train_ds):\n",
    "#         train_step(inp, tar)\n",
    "#         if (batch+1) %100==0:\n",
    "#             print('{}/{}'.format(batch+1,steps_per_epoch), end=\"\\r\")\n",
    "        \n",
    "#     print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "#           epoch + 1, train_loss.result(), train_acc.result()))\n",
    "# #     s=[tf.reduce_sum(w).numpy() for w in classifier.weights  if w.shape!=[]  ]\n",
    "# #     print(np.sum(s))\n",
    "#     if (epoch + 1) % 1 == 0:\n",
    "#         for (batch, (inp, tar)) in enumerate(val_ds):\n",
    "#             valid_step(inp, tar)\n",
    "#         lr_schedule.check(valid_loss.result())\n",
    "#         print ('Epoch {} ValLoss {:.4f} ValAccuracy {:.4f}'.format(\n",
    "#           epoch + 1,valid_loss.result() , valid_acc.result()))\n",
    "        \n",
    "#         if valid_loss.result()<best_val_loss:\n",
    "#             best_val_loss = valid_loss.result()\n",
    "#             ckpt_save_path = ckpt_manager.save()\n",
    "#             print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "#                                                              ckpt_save_path))\n",
    "#     print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 24.9421 Accuracy 0.7027\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-7a649920359b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mvalid_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         print ('Epoch {} ValLoss {:.4f} ValAccuracy {:.4f}'.format(\n",
      "\u001b[0;32m~/anaconda37/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda37/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda37/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss=np.inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    reset_stats()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(train_ds):\n",
    "        train_step(inp, tar)\n",
    "        if (batch+1) %100==0:\n",
    "            print('{}/{}'.format(batch+1,steps_per_epoch), end=\"\\r\")\n",
    "        \n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, train_loss.result(), train_acc.result()))\n",
    "#     s=[tf.reduce_sum(w).numpy() for w in classifier.weights  if w.shape!=[]  ]\n",
    "#     print(np.sum(s))\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        for (batch, (inp, tar)) in enumerate(val_ds):\n",
    "            valid_step(inp, tar)\n",
    "        \n",
    "        print ('Epoch {} ValLoss {:.4f} ValAccuracy {:.4f}'.format(\n",
    "          epoch + 1,valid_loss.result() , valid_acc.result()))\n",
    "        \n",
    "        if valid_loss.result()<best_val_loss:\n",
    "            best_val_loss = valid_loss.result()\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import time \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optmizer\n",
    "\n",
    "class SentimentPredictor(tf.keras.Model):\n",
    "    def __init__(self,drop_out_rate=0.1,n_classes=5,\n",
    "                 tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "                 tfhub_handle_encoder    = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'):\n",
    "        super(SentimentPredictor, self).__init__()\n",
    "        self.n_classes=n_classes\n",
    "        self.preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "        self.encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=False, name='BERT_encoder')\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(drop_out_rate)\n",
    "        self.out_layer = tf.keras.layers.Dense(n_classes, activation=None, name='classifier')\n",
    "\n",
    "    def call(self, inp):\n",
    "        self.encoder_inputs = self.preprocessing_layer(inp)\n",
    "        self.bert_out = self.encoder(self.encoder_inputs)['pooled_output']\n",
    "        x =self.dropout_layer(self.bert_out)\n",
    "        self.final_output = self.out_layer(x)\n",
    "        return self.final_output\n",
    "\n",
    "\n",
    "classifier = SentimentPredictor()\n",
    "\n",
    "optimizer = optimization.create_optimizer(init_lr=0.1,\n",
    "                                          num_train_steps=10,\n",
    "                                          num_warmup_steps=1,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SentimentPredictor at 0x7fe17d69fa58>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe0a0040ac8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(classifier=classifier,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       "array([[-1.1011397 , -1.5700848 ,  0.21170707,  0.35314897,  2.1451359 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# for (batch, (inp, tar)) in enumerate(val_ds):\n",
    "#         valid_step(inp, tar)\n",
    "        \n",
    "# print ('Epoch {} ValLoss {:.4f} ValAccuracy {:.4f}'.format(\n",
    "#                           epoch + 1,valid_loss.result() , valid_acc.result()))\n",
    "\n",
    "classifier([\"The result is good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./checkpoints/train/ckpt-4'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # np.log(0.95)+np.log(0.1)\n",
    "\n",
    "# y_true = [1, 2]\n",
    "# y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
    "# # Using 'auto'/'sum_over_batch_size' reduction type.\n",
    "# scce = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\n",
    "# print(scce(y_true, y_pred).numpy())\n",
    "\n",
    "# scce2 = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.SUM)\n",
    "# scce2(y_true, y_pred).numpy()\n",
    "\n",
    "\n",
    "# valid_acc.reset_states()\n",
    "# for (batch, (inp, tar)) in enumerate(val_ds):\n",
    "#     valid_step(inp, tar)\n",
    "# print ( valid_acc.result())\n",
    "\n",
    "ckpt_manager.latest_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function= lambda real, pred: tf.reduce_sum(loss_object(real, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code transforms the json reviews orginial data into a a new smaller file with fewer attributes for simplicity ( in order not to implement them all in  java objectMapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def process(txt):\n",
    "    txt = ''.join(txt.split('\\n'))\n",
    "    return txt.replace(\"\\\"\",\"\").replace(\"\\'\",\"\")\n",
    "\n",
    "lst_keep=[\n",
    "        'overall',\n",
    "        'reviewerID',\n",
    "        'asin',\n",
    "        'reviewText',\n",
    "        'unixReviewTime']\n",
    "\n",
    "dest_json=open(\"/home/hani/Desktop/tmp/reviews2.json\",'w')\n",
    "f=open(\"/home/hani/Desktop/tmp/reviews.json\")\n",
    "\n",
    "l = f.readline()\n",
    "while l:\n",
    "    \n",
    "    res=json.loads(l)\n",
    "    lst_k=list(res.keys()).copy()\n",
    "    for k in lst_k:\n",
    "        if k not in lst_keep:\n",
    "            del res[k]\n",
    "    txt= \"\\\"overall\\\": {0}, \\\"reviewerID\\\": \\\"{1}\\\", \\\"asin\\\": \\\"{2}\\\", \\\"reviewText\\\": \\\"{3}\\\", \\\"unixReviewTime\\\": \\\"{4}\\\"\".format(\n",
    "                                                                                                                              res[\"overall\"],\n",
    "                                                                                                                              res[\"reviewerID\"],\n",
    "                                                                                                                              res[\"asin\"],\n",
    "                                                                                                                              process(res[\"reviewText\"]),\n",
    "                                                                                                                              res[\"unixReviewTime\"])\n",
    "    dest_json.write(\"{\"+txt+\"}\\n\")\n",
    "    l = f.readline()\n",
    "f.close()\n",
    "dest_json.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
